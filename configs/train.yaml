# Training configuration
training:
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  num_epochs: 3
  warmup_steps: 100
  weight_decay: 0.01
  max_grad_norm: 1.0
  
# Optimizer settings
optimizer:
  name: "adamw"
  betas: [0.9, 0.999]
  eps: 1e-8

# Scheduler settings
scheduler:
  name: "cosine"
  num_warmup_steps: 100
  num_training_steps: 1000

# Validation settings
validation:
  eval_steps: 500
  save_steps: 1000
  eval_strategy: "steps"
  save_strategy: "steps"
  save_total_limit: 3

# Logging settings
logging:
  log_level: "info"
  use_wandb: true
  wandb_project: "llm-finance-predictor"
  log_steps: 100