{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Fine-tuning Demo for LLM Finance Predictor\n",
    "\n",
    "This notebook demonstrates how to quickly fine-tune a language model for financial prediction tasks.\n",
    "\n",
    "## Overview\n",
    "- Load and prepare financial data\n",
    "- Setup model and training configuration\n",
    "- Fine-tune the model with LoRA\n",
    "- Evaluate model performance\n",
    "- Make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "PyTorch version: 2.6.0+cpu\n",
      "CUDA available: False\n",
      "ðŸŒ Running on CPU environment\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"ðŸš€ Running on GPU environment\")\n",
    "else:\n",
    "    print(\"ðŸŒ Running on CPU environment\")\n",
    "    # Optional: Force Trainer to behave on CPU\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom modules\n",
    "sys.path.append('../src')\n",
    "from data.loader import FinancialDataLoader\n",
    "from data.preprocess import FinancialDataPreprocessor\n",
    "from data.dataset import FinancialDataModule\n",
    "from model.finetune import FinancialModelTrainer, FinancialModelEvaluator\n",
    "from model.inference import FinancialPredictionPipeline\n",
    "from utils.logging import FinancialLogger\n",
    "from utils.metrics import FinancialMetrics\n",
    "from utils.prompt_templates import FinancialPromptBuilder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup\n",
    "\n",
    "Let's create a configuration for our quick demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo configuration created:\n",
      "Model: microsoft/DialoGPT-small\n",
      "LoRA enabled: True\n",
      "Training epochs: 1\n",
      "Batch size: 4\n"
     ]
    }
   ],
   "source": [
    "# Create demo configuration\n",
    "demo_config = {\n",
    "    'model': {\n",
    "        'name': 'microsoft/DialoGPT-small',  # Using small model for demo\n",
    "        'size': 'small',\n",
    "        'quantization': None,\n",
    "        'max_length': 256,\n",
    "        'temperature': 0.7,\n",
    "        'top_p': 0.9,\n",
    "        'top_k': 50,\n",
    "        'architecture': {\n",
    "            'use_lora': True,\n",
    "            'lora_rank': 8,  # Smaller rank for demo\n",
    "            'lora_alpha': 16,\n",
    "            'lora_dropout': 0.1,\n",
    "            'target_modules': [\"c_attn\", \"c_proj\"]\n",
    "        }\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': 4,  # Small batch for demo\n",
    "        'gradient_accumulation_steps': 2,\n",
    "        'learning_rate': 5e-4,\n",
    "        'num_epochs': 1,  # Single epoch for demo\n",
    "        'warmup_steps': 5,\n",
    "        'weight_decay': 0.01,\n",
    "        'max_grad_norm': 1.0\n",
    "    },\n",
    "    'validation': {\n",
    "        'eval_steps': 50,\n",
    "        'save_steps': 100,\n",
    "        'eval_strategy': 'steps',\n",
    "        'save_strategy': 'steps',\n",
    "        'save_total_limit': 2\n",
    "    },\n",
    "    'logging': {\n",
    "        'log_level': 'info',\n",
    "        'use_wandb': True,  # Disabled for demo\n",
    "        'log_steps': 1\n",
    "    },\n",
    "    'use_retrieval': True,  # Disabled for demo\n",
    "    'target_column': 'price_direction_1d',\n",
    "    'max_length': 256,\n",
    "    'include_context': False\n",
    "}\n",
    "\n",
    "print(\"Demo configuration created:\")\n",
    "print(f\"Model: {demo_config['model']['name']}\")\n",
    "print(f\"LoRA enabled: {demo_config['model']['architecture']['use_lora']}\")\n",
    "print(f\"Training epochs: {demo_config['training']['num_epochs']}\")\n",
    "print(f\"Batch size: {demo_config['training']['batch_size']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Sample Data\n",
    "\n",
    "Let's create some sample financial data for the demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data created:\n",
      "Shape: (81, 13)\n",
      "Date range: 2023-01-20 00:00:00 to 2023-04-10 00:00:00\n",
      "Target distribution: {'False': 43, 'True': 38}\n",
      "\n",
      "Sample data:\n",
      "         date      Close      SMA_20        RSI price_direction_1d\n",
      "19 2023-01-20  92.122658  102.003501  33.992985               True\n",
      "20 2023-01-21  94.823047  101.744653  40.575612              False\n",
      "21 2023-01-22  94.394871  101.478223  48.438196               True\n",
      "22 2023-01-23  94.522357  101.153578  41.306258              False\n",
      "23 2023-01-24  91.828946  100.540412  58.929370              False\n"
     ]
    }
   ],
   "source": [
    "# Create sample financial data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "dates = pd.date_range('2023-01-01', periods=n_samples, freq='D')\n",
    "\n",
    "# Generate sample price data\n",
    "base_price = 100\n",
    "price_changes = np.random.randn(n_samples) * 0.02\n",
    "prices = [base_price]\n",
    "for change in price_changes[1:]:\n",
    "    prices.append(prices[-1] * (1 + change))\n",
    "\n",
    "# Create sample DataFrame\n",
    "sample_data = pd.DataFrame({\n",
    "    'symbol': ['AAPL'] * n_samples,\n",
    "    'date': dates,\n",
    "    'Open': [p * (1 + np.random.randn() * 0.01) for p in prices],\n",
    "    'High': [p * (1 + abs(np.random.randn()) * 0.01) for p in prices],\n",
    "    'Low': [p * (1 - abs(np.random.randn()) * 0.01) for p in prices],\n",
    "    'Close': prices,\n",
    "    'Volume': np.random.randint(1000000, 10000000, n_samples),\n",
    "    'SMA_20': pd.Series(prices).rolling(20).mean(),\n",
    "    'RSI': np.random.uniform(20, 80, n_samples),\n",
    "    'MACD': np.random.randn(n_samples) * 0.5,\n",
    "    'news_count': np.random.randint(0, 5, n_samples),\n",
    "    'sentiment_mean': np.random.uniform(-1, 1, n_samples)\n",
    "})\n",
    "\n",
    "# Create target variable (price direction for next day)\n",
    "sample_data['price_direction_1d'] = (sample_data['Close'].shift(-1) > sample_data['Close']).astype(str)\n",
    "sample_data = sample_data.dropna()\n",
    "\n",
    "print(f\"Sample data created:\")\n",
    "print(f\"Shape: {sample_data.shape}\")\n",
    "print(f\"Date range: {sample_data['date'].min()} to {sample_data['date'].max()}\")\n",
    "print(f\"Target distribution: {sample_data['price_direction_1d'].value_counts().to_dict()}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample data:\")\n",
    "print(sample_data[['date', 'Close', 'SMA_20', 'RSI', 'price_direction_1d']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Data Module\n",
    "\n",
    "Let's prepare the data for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split:\n",
      "Train: 56 samples\n",
      "Validation: 12 samples\n",
      "Test: 13 samples\n",
      "\n",
      "Data loaders created:\n",
      "Train batches: 14\n",
      "Val batches: 3\n",
      "Test batches: 4\n",
      "\n",
      "Sample batch:\n",
      "Input IDs shape: torch.Size([4, 512])\n",
      "Attention mask shape: torch.Size([4, 512])\n",
      "Labels shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Split data into train/val/test\n",
    "train_size = int(0.7 * len(sample_data))\n",
    "val_size = int(0.15 * len(sample_data))\n",
    "\n",
    "train_data = sample_data[:train_size]\n",
    "val_data = sample_data[train_size:train_size + val_size]\n",
    "test_data = sample_data[train_size + val_size:]\n",
    "\n",
    "print(f\"Data split:\")\n",
    "print(f\"Train: {len(train_data)} samples\")\n",
    "print(f\"Validation: {len(val_data)} samples\")\n",
    "print(f\"Test: {len(test_data)} samples\")\n",
    "\n",
    "# Initialize data module\n",
    "data_module = FinancialDataModule('../configs/base_model.yaml')\n",
    "\n",
    "# Setup tokenizer\n",
    "model_name = demo_config['model']['name']\n",
    "data_module.setup_tokenizer(model_name)\n",
    "\n",
    "# Prepare datasets\n",
    "data_module.prepare_datasets(\n",
    "    train_data, val_data, test_data,\n",
    "    retrieval_index=None  # No retrieval for demo\n",
    ")\n",
    "\n",
    "# Get data loaders\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = data_module.get_data_loaders(\n",
    "    batch_size=demo_config['training']['batch_size'],\n",
    "    num_workers=0,  # No multiprocessing for demo\n",
    "    shuffle_train=True\n",
    ")\n",
    "\n",
    "print(f\"\\nData loaders created:\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch:\")\n",
    "print(f\"Input IDs shape: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {sample_batch['attention_mask'].shape}\")\n",
    "print(f\"Labels shape: {sample_batch['labels'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model Trainer\n",
    "\n",
    "Let's setup the model trainer with our configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /home/riaarora/.netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mriaarora333\u001b[0m (\u001b[33mkorra141\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/riaarora/LLM_Finance_Predictor/notebooks/wandb/run-20260210_154957-igmt77ao</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/korra141/llm-finance-predictor/runs/igmt77ao' target=\"_blank\">comfy-aardvark-3</a></strong> to <a href='https://wandb.ai/korra141/llm-finance-predictor' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/korra141/llm-finance-predictor' target=\"_blank\">https://wandb.ai/korra141/llm-finance-predictor</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/korra141/llm-finance-predictor/runs/igmt77ao' target=\"_blank\">https://wandb.ai/korra141/llm-finance-predictor/runs/igmt77ao</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/microsoft/DialoGPT-small/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/microsoft/DialoGPT-small/49c537161a457d5256512f9d2d38a87d81ae0f0e/config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/microsoft/DialoGPT-small/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/microsoft/DialoGPT-small/49c537161a457d5256512f9d2d38a87d81ae0f0e/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/microsoft/DialoGPT-small/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/microsoft/DialoGPT-small/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/microsoft/DialoGPT-small/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/microsoft/DialoGPT-small/49c537161a457d5256512f9d2d38a87d81ae0f0e/config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/microsoft/DialoGPT-small/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/microsoft/DialoGPT-small/49c537161a457d5256512f9d2d38a87d81ae0f0e/config.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba1190f0a614982a8dbbbd7f1aee348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGPT2ForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/DialoGPT-small\n",
      "Key                              | Status     | \n",
      "---------------------------------+------------+-\n",
      "transformer.h.{0...11}.attn.bias | UNEXPECTED | \n",
      "lm_head.weight                   | UNEXPECTED | \n",
      "score.weight                     | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "WARNING:bitsandbytes.backends.cpu.ops:Failed to load CPU gemm_4bit_forward from kernels-community: No module named 'kernels'. Please make sure you already `pip install kernels` and the kernels >= 0.11.1\n",
      "INFO:model.finetune:Initialized model: microsoft/DialoGPT-small\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 812,544 || all params: 125,253,888 || trainable%: 0.6487\n",
      "Model information:\n",
      "  model_name: microsoft/DialoGPT-small\n",
      "  model_size: small\n",
      "  use_lora: True\n",
      "  trainable_parameters: 812544\n",
      "  total_parameters: 125253888\n",
      "\n",
      "Trainable parameters:\n",
      "trainable params: 812,544 || all params: 125,253,888 || trainable%: 0.6487\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer = FinancialModelTrainer(demo_config)\n",
    "\n",
    "# Get model info\n",
    "model_info = trainer.get_model_info()\n",
    "print(\"Model information:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Check if LoRA is properly applied\n",
    "if hasattr(trainer.model, 'print_trainable_parameters'):\n",
    "    print(f\"\\nTrainable parameters:\")\n",
    "    trainer.model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "\n",
    "Let's start the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Output directory: ./models/demo_model_20260210_155057\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
    "\n",
    "output_dir = f\"./models/demo_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Starting training...\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model.finetune:Starting training...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Train the model\n",
    "trained_trainer = trainer.train(\n",
    "    train_dataset=data_module.train_dataset,\n",
    "    eval_dataset=data_module.val_dataset,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Model\n",
    "\n",
    "Let's evaluate the trained model on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.load_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(iter(test_loader))\n",
    "# input_ids = sample['input_ids'].to(trainer.model.device)\n",
    "# attention_mask = sample['attention_mask'].to(trainer.model.device)\n",
    "# labels = sample['labels'].to(trainer.model.device)\n",
    "\n",
    "# outputs = trainer.model(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = FinancialModelEvaluator(trainer.model, trainer.tokenizer)\n",
    "\n",
    "print(\"Starting Evalu1ation\")\n",
    "\n",
    "# Evaluate direction accuracy\n",
    "eval_results = evaluator.evaluate_direction_accuracy(test_loader)\n",
    "\n",
    "print(\"Evaluation results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "metrics = FinancialMetrics()\n",
    "\n",
    "# Get predictions for detailed analysis\n",
    "trainer.model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(trainer.model.device)\n",
    "        attention_mask = batch['attention_mask'].to(trainer.model.device)\n",
    "        labels = batch['labels'].to(trainer.model.device)\n",
    "        \n",
    "        outputs = trainer.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        mask = labels != -100\n",
    "        if mask.any():\n",
    "            all_predictions.extend(predictions[mask].cpu().numpy())\n",
    "            all_labels.extend(labels[mask].cpu().numpy())\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "comprehensive_metrics = metrics.calculate_direction_accuracy(all_labels, all_predictions)\n",
    "\n",
    "print(f\"\\nComprehensive metrics:\")\n",
    "for key, value in comprehensive_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Make Predictions\n",
    "\n",
    "Let's test the model with some sample predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize prediction pipeline\n",
    "pipeline = FinancialPredictionPipeline(output_dir, None)\n",
    "\n",
    "# Create sample features for prediction\n",
    "sample_features = {\n",
    "    'symbol': 'AAPL',\n",
    "    'date': '2023-12-01',\n",
    "    'Close': 150.0,\n",
    "    'SMA_20': 148.5,\n",
    "    'RSI': 65.0,\n",
    "    'MACD': 0.5,\n",
    "    'news_count': 3,\n",
    "    'sentiment_mean': 0.2\n",
    "}\n",
    "\n",
    "# Make prediction\n",
    "result = pipeline.run_prediction_pipeline(sample_features, include_explanation=True)\n",
    "\n",
    "print(\"Prediction result:\")\n",
    "print(f\"Direction: {result['prediction']['prediction']['direction']}\")\n",
    "print(f\"Confidence: {result['prediction']['prediction']['confidence']}\")\n",
    "print(f\"Explanation: {result['prediction']['prediction']['explanation']}\")\n",
    "\n",
    "# Test with different features\n",
    "sample_features2 = {\n",
    "    'symbol': 'AAPL',\n",
    "    'date': '2023-12-02',\n",
    "    'Close': 145.0,\n",
    "    'SMA_20': 150.0,\n",
    "    'RSI': 35.0,\n",
    "    'MACD': -0.3,\n",
    "    'news_count': 1,\n",
    "    'sentiment_mean': -0.5\n",
    "}\n",
    "\n",
    "result2 = pipeline.run_prediction_pipeline(sample_features2, include_explanation=True)\n",
    "\n",
    "print(f\"\\nSecond prediction:\")\n",
    "print(f\"Direction: {result2['prediction']['prediction']['direction']}\")\n",
    "print(f\"Confidence: {result2['prediction']['prediction']['confidence']}\")\n",
    "print(f\"Explanation: {result2['prediction']['prediction']['explanation']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Predictions\n",
    "\n",
    "Let's test batch predictions on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for batch prediction\n",
    "feature_columns = [col for col in test_data.columns if col not in ['symbol', 'date', 'price_direction_1d']]\n",
    "features_list = []\n",
    "\n",
    "for _, row in test_data.iterrows():\n",
    "    features = {col: row[col] for col in feature_columns}\n",
    "    features_list.append(features)\n",
    "\n",
    "print(f\"Prepared {len(features_list)} samples for batch prediction\")\n",
    "\n",
    "# Make batch predictions\n",
    "batch_results = pipeline.run_batch_pipeline(features_list, include_explanations=False)\n",
    "\n",
    "# Analyze results\n",
    "predictions = [result['prediction']['prediction']['direction'] for result in batch_results]\n",
    "true_labels = test_data['price_direction_1d'].values\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = sum(1 for pred, true in zip(predictions, true_labels) \n",
    "              if (pred == 'UP' and true == 1) or (pred == 'DOWN' and true == 0))\n",
    "accuracy = correct / len(predictions)\n",
    "\n",
    "print(f\"\\nBatch prediction results:\")\n",
    "print(f\"Total predictions: {len(predictions)}\")\n",
    "print(f\"Correct predictions: {correct}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nSample predictions:\")\n",
    "for i in range(min(5, len(predictions))):\n",
    "    print(f\"Sample {i+1}: Predicted {predictions[i]}, Actual {true_labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "Let's summarize what we've accomplished and discuss next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== QUICK FINE-TUNING DEMO SUMMARY ===\")\n",
    "print(f\"Model: {demo_config['model']['name']}\")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"Training epochs: {demo_config['training']['num_epochs']}\")\n",
    "print(f\"LoRA enabled: {demo_config['model']['architecture']['use_lora']}\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"Direction accuracy: {eval_results['direction_accuracy']:.4f}\")\n",
    "print(f\"Batch prediction accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nModel saved to: {output_dir}\")\n",
    "\n",
    "print(f\"\\n=== NEXT STEPS ===\")\n",
    "print(\"1. Use more data for better performance\")\n",
    "print(\"2. Experiment with different model architectures\")\n",
    "print(\"3. Add retrieval-augmented generation (RAG)\")\n",
    "print(\"4. Implement more sophisticated prompt engineering\")\n",
    "print(\"5. Add more evaluation metrics\")\n",
    "print(\"6. Deploy the model for real-time predictions\")\n",
    "\n",
    "print(f\"\\n=== DEMO COMPLETED SUCCESSFULLY ===\")\n",
    "print(\"The LLM Finance Predictor has been fine-tuned and is ready for use!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM finetune python 3.10",
   "language": "python",
   "name": "llm_finetune_3_10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
